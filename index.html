<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="noarchive">
  <title>Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</title>
    <link rel="icon" href="assets/icon.jpg">
    <link rel="stylesheet" href="style.css">
<style>.authors, .authors a, .authors sup { color:#fff; }</style>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">
</head>
<body>
  <div class="page-container">
    
    <!-- Title Section -->
    <header class="paper-header">
      <h1 class="paper-title">Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</h1>
      
      <!-- Authors -->
      <div class="authors">
        <span class="author"><a href="https://www.linkedin.com/in/assaf-singer/">Anran Qi</a><sup>1</sup></span>
        <span class="author"><a href="https://rotsteinnoam.github.io/">Changjian Li</a><sup>2</sup></span>
        <span class="author"><a href="https://www.linkedin.com/in/amir-mann-a890bb276/">Adrien Bousseau</a><sup>1</sup></span>
        <span class="author"><a href="https://orlitany.github.io/">Niloy J.Mitra</a><sup>3, 4</sup></span>
      </div>

    <!-- Institutions -->
    <div class="affiliations">
      <span class="affiliation"><sup>1</sup>Inria - Université Côte d'Azur</span>
      <span class="affiliation"><sup>2</sup>University of Edinburgh</span>
        <span class="affiliation"><sup>3</sup>Adobe Research</span>
        <span class="affiliation"><sup>4</sup>University College London</span>
    </div>


      <!-- Links -->
      <div class="paper-links">
      <!-- Paper (PDF) -->
      <a href="./assets/paper_compress.pdf" class="paper-btn"  target="_blank" rel="noopener">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" aria-hidden="true">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
        </svg>
        Paper (PDF)
      </a>

      <!-- arXiv -->
      <a href="./assets/paper_compress.pdf" class="paper-btn" target="_blank" rel="noopener">
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" aria-hidden="true">
          <path d="M18 13v6a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
          <polyline points="15 3 21 3 21 9"></polyline>
          <line x1="10" y1="14" x2="21" y2="3"></line>
        </svg>
        arXiv
      </a>
        <a href="https://github.com/AnranQi/Rags2riches" class="paper-btn">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
          </svg>
          Code (Coming Soon)
        </a>
        <a href="./assets/sup_compress.pdf" class="paper-btn">
          <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect>
            <line x1="9" y1="3" x2="9" y2="21"></line>
          </svg>
          Supplementary
        </a>
      </div>
    </header>

    <!-- Teaser Video -->
    <section class="teaser-section">
      <div class="teaser-video-container">
        <video class="teaser-video" autoplay loop muted playsinline controls>
          <source src="assets/teaser.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </section>

    <!-- Abstract -->
    <section class="abstract-section">
    <h2>Abstract</h2>
    <p class="abstract-text" style="line-height:1.28;margin:0;">
        We address image-to-video generation with explicit user control over the <strong>final frame’s disoccluded regions</strong>. Current image-to-video pipelines produce plausible motion003
        but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to <strong>separate motion specification from
        appearance synthesis</strong>: we introduce a <strong>lightweight, user-editable Proxy Dynamic Graph (PDG)</strong> that deterministically yet approximately drives part motion, while a frozen009
        diffusion prior is used to synthesize plausible appearance that follows that motion. In our <strong>training-free</strong> pipeline, the
        user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit
        the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without finetuning. We demonstrate clear advantages against state-of-020
        the-art alternatives towards images turned into short videos of <strong>articulated objects, furniture, vehicles, and deformables</strong>.
        Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow.


    </p>
    </section>

    <!-- Motivation -->
    <section class="method-section">
      <h2>Motivation</h2>
      <div class="method-figure">
        <img src="assets/motivation.png" alt="Motivation" class="method-img">
      </div>
      
      <div class="method-description" style="line-height:1.28;margin:0;">
        <p>
        Motivation: forward vs. backward inconsistency in disocclusions. (a) Forward pass with DaS<a href="#ref1">[1]</a> exposes a large disoccluded region (white mask); (b) Backward pass from the
        last frame produces a plausible but different reveal resulting in a (c) difference map highlights misalignments concentrated on newly visible areas (rear of the bus, background),
        showing that <strong>forward/ backward visibility disagree</strong>. Hence, (d) Naive pixel copy–paste between forward/backward passes creates <strong>seams/ghosting (yellow circles)</strong> due to parallax, shading,
        and occlusion-order mismatches. This motivates us to solve the disocclusion problem.
        </p>
      </div>
      </section>

      <!-- Method Overview -->
    <section class="method-section">
      <h2>Method Overview</h2>
      <div class="method-figure">
        <img src="assets/method.png" alt="Method Overview" class="method-img">
      </div>

      <div class="method-description" style="line-height:1.28;margin:0;">
        <p>
         From an input image, we build a Proxy Dynamic Graph (PDG) and obtain coarse part tracks (marked with red arrows)
         and a disocclusion mask. <strong>Pass I</strong>, top: we run DaS (Diffusion-as-shader) to generate a motion-aware video driven by the PDG. The user
         then edits the final frame to prescribe the desired reveal in disoccluded regions (top-right). <strong>Pass II</strong>, bottom: without any retraining, we
         surgically replace the corresponding feature channels with the edited final-frame features and rerun DaS, yielding a video that preserves
         PDG-driven motion while matching the user-specified reveal (bottom-right).
        </p>
      </div>

      </section>



    <!-- User-Specified Object Control (plays 81 frames @ 24 fps) -->
    <section id="UserObjectControl" class="results-section">
      <h2>Part Motion Editing</h2>
      <p class="section-description" style="line-height:1.28;margin:0;">
        Examples of PDG-based object part motion editing.  we first estimate the depth map, the camera parameters and the objects or object parts segments. This allows us to lift the depth map into a point cloud for
        each part, which form the nodes of the PDG. Based on the relationships and motion parameters of each nodes,
        we transform the 3D pointcloud across all frames to render a coarse, warped version of the intended object animation (Left, Rendered).
        (Right, Ours) Our method generates a high-quality, dynamic video that accurately reflects the user’s intent.
      </p>
      <br>
      <div class="video-grid-2col">
        <!-- Header row: two header cells, one per column -->
        <div class="video-col-headercell">
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Rendered</span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Ours</span>
          <span class="spacer" aria-hidden="true"></span>
        <span style="display:block;height:calc(1em * 1.28 * 0.45)" aria-hidden="true"></span>
        </div>
        <div class="video-col-headercell">
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Rendered</span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Ours</span>
          <span class="spacer" aria-hidden="true"></span>
          <span style="display:block;height:calc(1em * 1.28 * 0.45)" aria-hidden="true"></span>
        </div>
        <!-- <div class="video-item">
          <video src="UserObjectControl/monkeyjumpingonthebed_concatenated_fixed.mp4" autoplay loop muted playsinline controls></video>
        </div> -->
        <div class="video-item">
            <video src="Part_Motion/toaster1.mp4" autoplay loop muted playsinline controls></video>
        </div>
        <div class="video-item">
            <video src="Part_Motion/toaster2.mp4" autoplay loop muted playsinline controls></video>
        </div>
        <div class="video-item">
          <video src="Part_Motion/art_lamp1.mp4" autoplay loop muted playsinline controls></video>
        </div>
        <div class="video-item">
          <video src="Part_Motion/art_lamp2.mp4" autoplay loop muted playsinline controls></video>
        </div>

      </div>
    </section>


    <!-- User-Specified Camera Control (plays 81 frames @ 16 fps) -->
    <section id="UserCameraControl" class="results-section">
      <h2>Disocclusion and Part Motion Editing</h2>
      <p class="section-description" style="line-height:1.28;margin:0;">
        Examples of PDG-based object part motion and disocclusion editing. The motion of the objects/parts inevitably reveals disoccluded regions, we reinstate control over these regions (Right, Ours).
      </p>
      <br>
      <div class="video-grid-2col">
                <div class="video-col-headercell">
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Rendered + New Concept</span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Ours</span>
          <span class="spacer" aria-hidden="true"></span>
        <span style="display:block;height:calc(1em * 1.28 * 0.45)" aria-hidden="true"></span>
        </div>
        <div class="video-col-headercell">
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Rendered + New Concept</span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="spacer" aria-hidden="true"></span>
          <span class="label">Ours</span>
          <span class="spacer" aria-hidden="true"></span>
          <span style="display:block;height:calc(1em * 1.28 * 0.45)" aria-hidden="true"></span>
        </div>
        <div class="video-item">
            <video src="Part_Motion_Disocclusion/laptop3.mp4" autoplay loop muted playsinline controls></video>
            <p class="video-label">Rendered | Ours</p>
        </div>
        <div class="video-item">
          <video src="Part_Motion_Disocclusion/lantern.mp4" autoplay loop muted playsinline controls></video>
          <p class="video-label">Rendered | Ours</p>
        </div>
        <div class="video-item">
            <video src="Part_Motion_Disocclusion/dog.mp4" autoplay loop muted playsinline controls></video>
            <p class="video-label">Rendered | Ours</p>
        </div>
        <div class="video-item">
            <video src="Part_Motion_Disocclusion/closet.mp4" autoplay loop muted playsinline controls></video>
            <p class="video-label">Rendered | Ours</p>
        </div>
        <div class="video-item">
            <video src="Part_Motion_Disocclusion/gingerbread.mp4" autoplay loop muted playsinline controls></video>
            <p class="video-label">Rendered | Ours</p>
        </div>

        <div class="video-item">
            <video src="Part_Motion_Disocclusion/box.mp4" autoplay loop muted playsinline controls></video>
            <p class="video-label">Rendered | Ours</p>
        </div>

      </div>
    </section>

    <!-- Qualitative Comparison - Part Motion Control (plays 81 frames @ 24 fps)  -->
    <section class="comparison-section">
      <h2>Qualitative Comparison: Part Motion Editing</h2>
      <p class="section-description" style="line-height:1.28;margin:0;">
    Comparison of our method with the state-of-the-art motion control method Veo3.1<a href="#ref1">[2]</a>, DragAnything<a href="#ref2">[3]</a> and Puppet Master<a href="#ref2">[4]</a> on user-specified object part editing tasks.
      </p>
      <!-- Videos - Hamburger, Snooker, Find Other -->
      <div class="comparison-grid">
        <div class="video-col-headercell-small">
          <span class="label">Rendered</span>
          <span class="label">Ours</span>
          <span class="label">Veo3.1</span>
          <span class="label">DragAnything</span>
          <span class="label">Puppet Master</span>
        </div>


        <div class="comparison-item">
          <video src="Part_Motion_Comparison/motion_toaster_compare.mp4" autoplay loop muted playsinline controls></video>
        </div>
        <div class="comparison-item">
          <video src="Part_Motion_Comparison/motion_lamp_compare.mp4" autoplay loop muted playsinline controls></video>
        </div>

      </div>
    </section>

    <!-- Qualitative Comparison - Camera Control (plays 81 frames @ 16 fps) -->
    <section class="comparison-section">
      <h2>Qualitative Comparison: Disocclusion and Part Motion Editing</h2>
      <p class="section-description" style="line-height:1.28;margin:0;">
        Comparison of our method on DDisocclusion and Part Motion Editing. Since no prior work addresses our novel motion- and disocclusion-aware video generation task, we created four baseline methods (see our paper for more detail).
      </p>

      <div class="comparison-grid">
        <div class="video-col-headercell-small-6">
          <span class="label">Rendered + New Concept</span>
          <span class="label">Ours</span>
          <span class="label">DaS_Tnew</span>
          <span class="label">pixel-cp</span>
          <span class="label">pixel-cp++</span>
          <span class="label">Veo3.1</span>
        </div>

        <div class="comparison-item">
          <video src="Part_Motion_Disocclusion_Comparison/motion_disocclusion_hat_compare.mp4" autoplay loop muted playsinline controls></video>
        </div>
        <div class="comparison-item">
          <video src="Part_Motion_Disocclusion_Comparison/motion_disocclusion_bus_compare.mp4" autoplay loop muted playsinline controls></video>
        </div>
      </div>
    </section>

    <!-- BibTeX -->
    <section class="bibtex-section">
      <h2>Citation</h2>
      <pre class="bibtex"><code>
      @misc{Qi2025beyondvisible,
            title={Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs},
            author={Anran Qi, Changjian Li, Adrien Bousseau and Miloy J.Mitra},
            year={2025},
            eprint={},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={},
      }
</code></pre>
    </section>

    <!-- References Section -->
      <section class="references-section">
        <h2>References</h2>
        <ol class="references-list">
          <li id="ref1">
            <span class="ref-authors">Gu, Zekai, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong et al. </span>,
            <span class="ref-title">"Diffusion as shader: 3d-aware video diffusion for versatile video generation control."</span>,
            <span class="ref-venue">Siggraph 2025</span>.
          </li>
          <li id="ref2">
            <span class="ref-authors">Google DeepMind. </span>,
            <span class="ref-title">"Veo: Video generation with moe-driven diffusion and audio generation."</span>,
            <span class="ref-venue">2024</span>.
          </li>

          <li id="ref3">
            <span class="ref-authors">Wu, Weijia, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang </span>,
            <span class="ref-title">"Draganything: Motion control for anything using entity representation."</span>,
            <span class="ref-venue">ECCV 2024</span>.
          </li>
          <li id="ref4">
            <span class="ref-authors">Li, Ruining, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi </span>,
            <span class="ref-title">"Puppet-master: Scaling interactive video generation as a motion prior for part-level dynamics."</span>,
            <span class="ref-venue">ICCV 2025</span>.
          </li>

          <!-- Add more references as needed -->
        </ol>
      </section>

    <!-- Footer (with Copyright & License Section) -->
    <footer class="page-footer">
    <p>
    <blockquote style="background:#f8f9fa;border-left:4px solid #ffffff;padding:1em 1.5em;border-radius:6px;font-size:1.05em;">
    Website template and code adapted from <a href="https://github.com/time-to-move/time-to-move.github.io">Time-to-Move Project Page</a> by Assaf Singer et al.
    </blockquote>
    </footer>
    
  </div>
</body>
</html>

<!-- Copyright & License Section
<section class="copyright-section">
  <h2>Copyright & License</h2>
  <p>
    The website code and design are © 2025 Assaf Singer and contributors. You are welcome to use, adapt, and share this project page for your own research or academic work. <br>
    <strong>If you use or adapt this website, please include the following acknowledgment:</strong>
  </p>
  <blockquote style="background:#f8f9fa;border-left:4px solid #667eea;padding:1em 1.5em;border-radius:6px;font-size:1.05em;">
    Website template and code adapted from <a href="https://github.com/time-to-move/time-to-move.github.io">Time-to-Move Project Page</a> by Assaf Singer et al.
  </blockquote>
  <p>
    For questions or collaboration, contact <a href="mailto:assafsin0@gmail.com">assafsin0@gmail.com</a>.<br>
    <strong>License:</strong> MIT License (see repository for details)
  </p>
  <footer class="page-footer">
    <p>© 2025 - Time-to-Move Project Page</p>
    <p>
      This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.<br>
      You are free to borrow the <a href="https://github.com/time-to-move/time-to-move.github.io">source code</a> of this website. We ask that you link back to this page in the footer or acknowledgments section of your project.<br>
      <!-- Please remember to remove any analytics code or scripts you do not want on your website.<br> -->
      <!-- For questions or collaboration, contact <a href="mailto:assafsin0@gmail.com">assafsin0@gmail.com</a>. -->
    <!-- </p>
    <p><a href="supplementary.html">View Supplementary Materials</a></p>
  </footer> -->
